{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ4gqbz24DV"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this assignment, you will build a language identification classifier that distinguishes between six languages:\n",
        "\n",
        "- [Hausa](https://en.wikipedia.org/wiki/Hausa_language)\n",
        "- [Indonesian](https://en.wikipedia.org/wiki/Indonesian_language)\n",
        "- [Manobo](https://en.wikipedia.org/wiki/Manobo_languages)\n",
        "- [Nahuatl](https://en.wikipedia.org/wiki/Nahuatl)\n",
        "- [Swahili](https://en.wikipedia.org/wiki/Swahili_language)\n",
        "- [Tagalog](https://en.wikipedia.org/wiki/Tagalog_language)\n",
        "\n",
        "Some languages can be distinguished easily, because they use different scripts. These six languages, however, use the same ([Latin](https://en.wikipedia.org/wiki/Latin_script)) script with minimal [diacritics](https://en.wikipedia.org/wiki/Diacritic) so it is difficult to hand-craft classifiers based on the presence or absence of particular characters. Indeed, unless you have linguistic training or familiarity with the languages, it is difficult to tell them apart.\n",
        "\n",
        "How can they be distinguished? A naïve approach is to use word counts as unigram features. However, the number of possible words in a large corpus of five languages is vast. It is essential to look at something smaller — characters.\n",
        "\n",
        "Even though the six languages use roughly the same characters, the relative frequencies of these characters vary greatly. Thus, using characters as features (unigram character models) is appealing (and fairly effective). It is also true that languages very in their *phonotactics*, the way in which consonants and vowels combine in sequence. Thus, looking at character ngrams (for small values of $n$) is also appealing (and effective). Note, however, that as the value of $n$ increases, this approach runs into the same problem as the word unigram model (sparcity). In this scenario, the model is likely to overfit.\n",
        "\n",
        "Various kinds of classifiers can be used for this application. NB classifiers, for example, are quite effective. However, inference is slow and performance, given the same training set, is likely to be worse than other options. Simple logistic regression cannot be used because this is an n-way (multinomial) classification problem. Multinomial Logistic Regression (Softmax Regression) is a good fit.\n",
        "\n",
        "## Summary\n",
        "\n",
        "You will perform the following tasks:\n",
        "\n",
        "1. Implement a training loop for Multinomial Logistic Regression.\n",
        "2. Implement inference for Multinomal Logistic Regression\n",
        "3. Determine the optimal order of $n$ for ngrams for MNLR trained on the training set.\n",
        "4. Calculate and display a confusion matrix for a trigram model evaluated on the test set.\n",
        "5. Inspect the feature weights, and display the most predictive features for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTpaLHk024DX"
      },
      "source": [
        "# Imports\n",
        "Do not change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1dK4MYN24DY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import numpy.testing as testing\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0iwikHR24DY"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxebtBh24DY"
      },
      "source": [
        "## Metrics for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozYPhBX324DZ"
      },
      "outputs": [],
      "source": [
        "def precision(tp: int, fp: int) -> float:\n",
        "    \"\"\"Computes the precision, given true positives and false positives.\"\"\"\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "\n",
        "def recall(tp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the recall, given the true positives and false negatives.\"\"\"\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def f_measure(beta: float, tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F-measure for a given beta, true positives, false positives, and false negatives.\"\"\"\n",
        "    return (\n",
        "        (1 + beta**2)\n",
        "        * (precision(tp, fp) * recall(tp, fn))\n",
        "        / (beta**2 * precision(tp, fp) * recall(tp, fn))\n",
        "    )\n",
        "\n",
        "\n",
        "def f1(tp: int, fp: int, fn: int) -> float:\n",
        "    \"\"\"Computes the F1 measure for a given TP, FP, and FN.\"\"\"\n",
        "    return f_measure(1, tp, fp, fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nELi0-b24DZ"
      },
      "source": [
        "## Micro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGvXgOq624DZ"
      },
      "outputs": [],
      "source": [
        "def micro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged precision.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fp_sum = sum(fp.values())\n",
        "    return tp_sum / (tp_sum + fp_sum)\n",
        "\n",
        "\n",
        "def micro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged recall.\"\"\"\n",
        "    tp_sum = sum(tp.values())\n",
        "    fn_sum = sum(fn.values())\n",
        "    return tp_sum / (tp_sum + fn_sum)\n",
        "\n",
        "\n",
        "def micro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes micro-averaged F1.\"\"\"\n",
        "    mp = micro_precision(tp, fp)\n",
        "    mr = micro_recall(tp, fn)\n",
        "    return 2 * (mp * mr) / (mp + mr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khOVHiDN24Da"
      },
      "source": [
        "## Macro-Averaged Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM64SpHj24Da"
      },
      "outputs": [],
      "source": [
        "def macro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged precision.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([precision(tp[c], fp[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged recall.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (1 / n) * sum([recall(tp[c], fn[c]) for c in tp.keys()])\n",
        "\n",
        "\n",
        "def macro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
        "    \"\"\"Computes macro-averaged F1.\"\"\"\n",
        "    n = len(tp)\n",
        "    return (\n",
        "        2\n",
        "        * (macro_precision(tp, fp) * macro_recall(tp, fn))\n",
        "        / (macro_precision(tp, fp) + macro_recall(tp, fn))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBID2gd024Da"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW0VMaqR24Da"
      },
      "source": [
        "## Loading data from files\n",
        "\n",
        "We first need to load data from the provided TSV files. Each file is two columns, the language of the document and the document text, separated by a tab (`\\t`) character. We load this data into a list of tuples, to maintain the coupling between each document and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUsQo8rV24Da"
      },
      "outputs": [],
      "source": [
        "def load_data(data_filename: str) -> list[tuple[str, str]]:\n",
        "    with open(data_filename) as fin:\n",
        "        reader = csv.reader(fin, delimiter=\"\\t\")\n",
        "        language_document_tuples = [(lang, doc) for (lang, doc) in reader]\n",
        "    return language_document_tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sVjmft724Da"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We will use ngrams as features, so we need to be able to extract them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H_J8TWP24Da"
      },
      "outputs": [],
      "source": [
        "def extract_ngrams(x: str, n=3) -> \"list[str]\":\n",
        "    \"\"\"Given a string, return all character ngrams of order `n`.\"\"\"\n",
        "    return [\"\".join(s) for s in (zip(*[x[i:] for i in range(n)]))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKPihtrW24Db"
      },
      "source": [
        "## Language Codes to One-Hot Vectors\n",
        "And we need a function to convert a language code into a **one-hot vector** (called $\\mathbf{y}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMfr0scq24Db"
      },
      "outputs": [],
      "source": [
        "def to_onehot_vector(lang: str, langs: list[str]) -> np.ndarray:\n",
        "    y = np.zeros(len(langs))\n",
        "    y[langs.index(lang)] = 1\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLC22gw724Db"
      },
      "source": [
        "We need to be able to convert `dict`s of ngram counts to vectors of ngram counts (using a map from ngrams to dimensions of the vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMqboGoj24Db"
      },
      "outputs": [],
      "source": [
        "def vectorize_ngrams(counter: dict[str, int], feature_map: dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a dict of ngram counts and a map from features to indices, returns a vector of ngram counts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter : dict\n",
        "        Counter/dict of ngram counts\n",
        "    feature_map : dict\n",
        "        Map from ngrams to indices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A vector of ngram counts\n",
        "    \"\"\"\n",
        "    feature_vector = np.zeros(len(feature_map))\n",
        "    for ngram, count in counter.items():\n",
        "        if ngram in feature_map:\n",
        "            feature_vector[feature_map[ngram]] = count\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZnsbPHW24Db"
      },
      "source": [
        "And putting together the conversion from text into ngrams, and vectorizing the ngrams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYKBp9qm24Db"
      },
      "outputs": [],
      "source": [
        "def vectorize_document(document: str, feature_map: dict[str, int], ngram_length: int) -> np.ndarray:\n",
        "    document_ngrams = extract_ngrams(document, ngram_length)\n",
        "    vector = vectorize_ngrams(Counter(document_ngrams), feature_map)\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPL24tz24Db"
      },
      "source": [
        "We need separate functions for preprocessing the training observations and the dev/test observations. The former function must return a map from language names to labels as one-hot vectors as well as a map from features (ngrams) to indices of vector dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqcXY2Gj24Db"
      },
      "outputs": [],
      "source": [
        "def preprocess_training_observations(\n",
        "    training_observations: list[tuple[str, str]], n: int = 1\n",
        ") -> tuple[list[tuple[np.ndarray, np.ndarray]], dict[str, np.ndarray], dict[str, int]]:\n",
        "    langs = set()\n",
        "    features = set()\n",
        "    obs = []\n",
        "\n",
        "    for lang, doc in training_observations:\n",
        "        langs.add(lang)\n",
        "        ngrams = extract_ngrams(doc, n)\n",
        "        features = features | set(ngrams)\n",
        "        obs.append((lang, ngrams))\n",
        "    feature_map = {feature: idx for idx, feature in enumerate(sorted(features))}\n",
        "    lang_list = list(sorted(langs))\n",
        "    lang_map = {lang: to_onehot_vector(lang, lang_list) for lang in lang_list}\n",
        "\n",
        "    obs = [\n",
        "        (lang_map[lang], vectorize_ngrams(Counter(ngrams), feature_map)) for (lang, ngrams) in obs\n",
        "    ]\n",
        "\n",
        "    print(f\"{len(obs)} training observations.\")\n",
        "    return obs, lang_map, feature_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynWFa0ij24Db"
      },
      "source": [
        "The function for preprocessing test observations (and dev observations) takes the feature map and the language map as arguments and returns only the list of labeled observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRC7QyTC24Db"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_observations(\n",
        "    test_observations, feature_map: dict[str, int], lang_map: dict[str, np.ndarray], n: int = 1\n",
        ") -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
        "    obs = []\n",
        "\n",
        "    for i, (lang, doc) in enumerate(test_observations):\n",
        "        vectorized_doc = vectorize_document(doc, feature_map, ngram_length=n)\n",
        "        try:\n",
        "            obs.append((lang_map[lang], vectorized_doc))\n",
        "        except KeyError:\n",
        "            print(f\"Unkown language {lang} at index {i}. Known languages are: {lang_map.keys()}\")\n",
        "    print(f\"{len(obs)} test observations.\")\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQoVLwtc24Db"
      },
      "source": [
        "## Classification Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5VsPMSZ24Db"
      },
      "source": [
        "We also need to define the softmax function.\n",
        "\n",
        "Softmax is technically\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum^K_{j=1} e^{z_j}}$$\n",
        "\n",
        "However, if implemented naïvely, this is not numerically stable. Instead, we use:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i - \\text{max}(z)}}{\\sum^K_{j=1} e^{z_j - \\text{max}(z)}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mrbr-ll24Dc"
      },
      "outputs": [],
      "source": [
        "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
        "    \"\"\"Compute the softmax of a vector `z`\"\"\"\n",
        "    # exp(z) can get very large. For numerical stability, we subtract a vector of very large values (np.max(z)) from z.\n",
        "    return np.exp(z - np.max(z)) / np.exp(z - np.max(z)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yasahYJv24Dc"
      },
      "source": [
        "# Training the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGVeehjS24Dc"
      },
      "source": [
        "### Compute the gradient\n",
        "\n",
        "The formula for computing one element in our gradient is as follows (the partial derivitive of the negative log likelihood loss):\n",
        "\n",
        "$$\\frac{\\partial L_{CE}}{\\partial \\mathbf{w}_{k,i}}=-(\\mathbf{y}_k-\\hat{\\mathrm{y}}_k)\\mathbf{x}_i$$\n",
        "\n",
        "where $k$ is the **class** (rows of the matrix $\\mathbf{w}$) and $i$ corresponds the the feature (columns of the matrix $\\mathbf{w}$).\n",
        "\n",
        "We will define a function `grad` for computing the whole gradient, a $K \\times N$ matrix.\n",
        "\n",
        "**This is the first piece of code that you'll write.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "Answer Expected"
        ],
        "id": "QbQvSLjG24Dc"
      },
      "outputs": [],
      "source": [
        "def grad(W: np.ndarray, y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Caculates the gradient of the negative log liklihood loss, a [K * N] matrix, with respect to W.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "       A matrix of of weights\n",
        "    y : np.ndarray\n",
        "       The true label of the observation, expressed as a [K * N] matrix.\n",
        "    x : np.ndarray\n",
        "       A vector of features.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The gradient of the loss with respect to W.\n",
        "    \"\"\"\n",
        "    K, N = W.shape  # K: number of classes, N: number of features\n",
        "    # Compute the logits\n",
        "    logits = np.dot(W, x)\n",
        "\n",
        "    # Apply softmax to get predicted probabilities for all classes at once\n",
        "    softmax_output = softmax(logits)\n",
        "\n",
        "    # Compute the gradient for each class and feature\n",
        "    grad_matrix = np.outer(softmax_output - y, x)\n",
        "\n",
        "    return grad_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2j-LrAj24Dc"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Iterate over the observations in the training set $n$ times (in random order). For each item, compute the one-hot vector $\\mathbf{y}$ and the probability distribution $\\hat{\\mathbf{y}}$. Use these values to compute the gradient. Update the parameters based on the gradient. At the end of each epoch (pass through the training data), compute the true positives, false positives, and false negatives for each target class based on the current weights, and report micro-averaged precision and recall.\n",
        "\n",
        "How you report the metrics is up to you — we will not look at what you output to STDOUT — but it is important that you do this. **Otherwise, you will not be able to determine whether your model is training.**\n",
        "\n",
        "You can also output the loss at each step (very noisy!), each epoch, or run the classifier on the dev set and report the metrics at the end of each epoch.\n",
        "\n",
        "Remember that the 0th column in $W$ contains the biases. You will have to insert a $1$ at the beginning of the feature vector $x$ in order to accomodate this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "Answer Expected"
        ],
        "id": "Li1XX1qe24Dc"
      },
      "outputs": [],
      "source": [
        "def train(observations: tuple[np.ndarray, np.ndarray], eta: float, epochs: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given a set of observations, returns a trained multinomial LR (softmax regression) model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    observations : (np.ndarray, np.ndarray)\n",
        "        Pairs consisting of a tuple of NumPy arrays (a one-hot vector encoding the ground truth language labels and a vector of features)\n",
        "    eta : float\n",
        "        The learning rate for parameter updates.\n",
        "    epochs : int\n",
        "        The number of epochs to train the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The model as a [K * N] weight matrix.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    y, X = zip(*observations)\n",
        "    y = np.array(y)\n",
        "    X = np.array(X)\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]  # Assuming y is already one-hot encoded\n",
        "\n",
        "    # Initialize the weight matrix W with small random values or zeros. Include +1 for the bias term.\n",
        "    W = np.random.randn(n_classes, n_features + 1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the dataset at the beginning of each epoch\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        total_loss = 0.0  # Accumulator for total loss in this epoch\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            x = X_shuffled[i]\n",
        "            true_y = y_shuffled[i]\n",
        "\n",
        "            # Insert a 1 at the beginning of the feature vector x for the bias term\n",
        "            x_bias = np.insert(x, 0, 1)\n",
        "\n",
        "            # Compute the gradient\n",
        "            gradient = grad(W, true_y, x_bias)\n",
        "\n",
        "            # Update the weights\n",
        "            W -= eta * gradient\n",
        "\n",
        "            # Compute loss for this sample and accumulate\n",
        "            loss = np.sum(-true_y * np.log(softmax(np.dot(W, x_bias))))\n",
        "            total_loss += loss\n",
        "\n",
        "        # Print average loss for this epoch\n",
        "        avg_loss = total_loss / n_samples\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss}\")\n",
        "\n",
        "\n",
        "        # Here, you can add code to compute and print metrics (precision, recall) after each epoch\n",
        "        # This might involve computing predictions for the training set and comparing to true_y\n",
        "\n",
        "    return W\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqNbxiZb24Dc"
      },
      "source": [
        "# Classification\n",
        "\n",
        "The classification function is very simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "Answer Expected"
        ],
        "id": "y4i_Tv6224Dc"
      },
      "outputs": [],
      "source": [
        "def classify(W: np.ndarray, x: np.ndarray) -> np.intp:\n",
        "    \"\"\"\n",
        "    Return the index of the hypothesized language.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W : np.ndarray\n",
        "        Weight matrix (one row for each category/language, on column for each feature)\n",
        "    x : np.ndarray\n",
        "        Vector of real-valuled features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    intp\n",
        "        The index of the hypothesized language.\n",
        "    \"\"\"\n",
        "    # Insert bias term\n",
        "    x = np.insert(x, 0, 1)\n",
        "\n",
        "    # Calculate probabilities using softmax\n",
        "    probabilities = softmax(np.dot(W, x))\n",
        "\n",
        "    # Return the index with the highest probability\n",
        "    return np.argmax(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-MNeElL24Dc"
      },
      "source": [
        "# Evaluate the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2wcJo5e24Dc"
      },
      "source": [
        "Then, a function to train and evaluate the classifier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train_set, test_set, eta, epochs=3):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a multinomial logistic regression model based on provided training and test datasets.\n",
        "    Includes safe calculation of micro and macro F1 scores to avoid division by zero errors.\n",
        "    \"\"\"\n",
        "    # Unpack the training and test sets\n",
        "    y_train, X_train = zip(*train_set)\n",
        "    y_train = np.array(y_train)\n",
        "    X_train = np.array(X_train)\n",
        "\n",
        "    y_test, X_test = zip(*test_set)\n",
        "    y_test = np.array(y_test)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    # Train the model\n",
        "    W = train(train_set, eta, epochs)\n",
        "\n",
        "    # Initialize metrics\n",
        "    tp = {i: 0 for i in range(y_test.shape[1])}\n",
        "    fp = {i: 0 for i in range(y_test.shape[1])}\n",
        "    fn = {i: 0 for i in range(y_test.shape[1])}\n",
        "\n",
        "    # Predict and update TP, FP, FN\n",
        "    for i in range(X_test.shape[0]):\n",
        "        x = X_test[i]\n",
        "        true_label = np.argmax(y_test[i])\n",
        "        predicted_label = classify(W, x)\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "            tp[predicted_label] += 1\n",
        "        else:\n",
        "            fp[predicted_label] += 1\n",
        "            fn[true_label] += 1\n",
        "\n",
        "    # Safely compute micro and macro F1 scores\n",
        "    try:\n",
        "        test_micro_f1 = micro_f1(tp, fp, fn)\n",
        "    except ZeroDivisionError:\n",
        "        test_micro_f1 = 0.0  # Default to 0 since it gives me division by zero error\n",
        "\n",
        "    try:\n",
        "        test_macro_f1 = macro_f1(tp, fp, fn)\n",
        "    except ZeroDivisionError:\n",
        "        test_macro_f1 = 0.0  # Default to 0 since it gives me division by zero error\n",
        "\n",
        "    print(f\"Micro F1: {test_micro_f1:.4f}\")\n",
        "    print(f\"Macro F1: {test_macro_f1:.4f}\")\n",
        "\n",
        "    return test_macro_f1, test_micro_f1\n"
      ],
      "metadata": {
        "id": "e37nZlkNJj_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NaLNKeS24Dc"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7_mJ7r124Dd"
      },
      "source": [
        "Load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKTp_vml24Dd"
      },
      "outputs": [],
      "source": [
        "train_observations = load_data(\"train.tsv\")\n",
        "test_observations = load_data(\"test.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14jQ9h3R24Dd"
      },
      "source": [
        "## Inspecting the data\n",
        "\n",
        "Before we actually train the classifier, it's important to look at your data, and check that any assumptions you're making about it are justified. It's always useful at this point to check basic things, like:\n",
        "- How many instances of train and test data do you have?\n",
        "- What labels are in your data, and do those match between the train and test splits?\n",
        "- What is the class balance (i.e. how many instances of each class) in your dataset? Is it balanced or unbalanced?\n",
        "\n",
        "Output a dictionary for the train and test data that maps each label to the count of instances that have that label, e.g.:\n",
        "```\n",
        "{\"hausa\": 4000, \"indonesian\":...}\n",
        "```\n",
        "\n",
        "Your dictionary should be sorted in descending order of occurrence of languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "1.1",
        "tags": [
          "Answer Expected"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Napa7w8w24Dd",
        "outputId": "bb8c3114-1d65-4461-a2b7-e677905efd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"swahili\": 377, \"tagalog\": 365, \"manobo\": 111, \"hausa\": 91, \"nahuatl\": 90, \"indonesian\": 86}\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.1\n",
        "# your code here - train set\n",
        "import json\n",
        "\n",
        "# Your existing train set code to count occurrences of each label\n",
        "label_counts = Counter(label for label, _ in train_observations)\n",
        "label_counts_dict = dict(label_counts)\n",
        "\n",
        "# Sort the dictionary in descending order of occurrences\n",
        "sorted_label_counts = dict(sorted(label_counts_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print the sorted dictionary in JSON format without indentation\n",
        "print(json.dumps(sorted_label_counts, indent=None))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "1.2",
        "tags": [
          "Answer Expected"
        ],
        "id": "mDP4jmqi24Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cdd8c6-e835-4490-a5c8-8e4b6a19785b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"swahili\": 142, \"tagalog\": 121, \"nahuatl\": 36, \"manobo\": 34, \"indonesian\": 32, \"hausa\": 28}\n"
          ]
        }
      ],
      "source": [
        "# this cell's output will be used for test 1.2\n",
        "# your code here - test set\n",
        "\n",
        "# Your existing train set code to count occurrences of each label\n",
        "label_counts = Counter(label for label, _ in test_observations)\n",
        "label_counts_dict = dict(label_counts)\n",
        "\n",
        "# Sort the dictionary in descending order of occurrences\n",
        "sorted_label_counts = dict(sorted(label_counts_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print the sorted dictionary in JSON format without indentation\n",
        "print(json.dumps(sorted_label_counts, indent=None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvvWHkzd24De"
      },
      "source": [
        "## Set hyperparameters and parameters\n",
        "\n",
        "Before training the model, we have to set the learning rate $\\eta$ and the order of the ngrams used in feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfsJ10U_24De"
      },
      "outputs": [],
      "source": [
        "eta = 0.0005  # Do not change this.\n",
        "epochs = 4  # Do not change this.\n",
        "order_of_ngrams = 1  # Do change this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DWNlBH024De"
      },
      "source": [
        "## Running our training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5552Td624De"
      },
      "source": [
        "Now train your classifier, and evaluate it on the test set. Vary the number of ngrams, and observe how it changes train and test F1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APHDpnHG24De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159b761f-5bcc-4b2e-86a7-a16450566bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "Epoch 1/4, Average Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-204-27e78a002b8f>:53: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-true_y * np.log(softmax(np.dot(W, x_bias))))\n",
            "<ipython-input-204-27e78a002b8f>:53: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-true_y * np.log(softmax(np.dot(W, x_bias))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/4, Average Loss: 6.484015063598121\n",
            "Epoch 3/4, Average Loss: 3.594608010156146\n",
            "Epoch 4/4, Average Loss: nan\n",
            "Micro F1: 0.7837\n",
            "Macro F1: 0.6722\n"
          ]
        }
      ],
      "source": [
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=order_of_ngrams\n",
        ")\n",
        "test_set = preprocess_test_observations(test_observations, feature_map, lang_map, n=order_of_ngrams)\n",
        "\n",
        "\n",
        "random.seed(27)\n",
        "test_macro_f1, test_micro_f1 = evaluate(train_set, test_set, eta, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78t9Zssz24De"
      },
      "source": [
        "Print the tuple of the best values of `(macro_f1, micro_f1)` for the model evaluated on your test set while varying the order of the ngrams. What value of n produced the best result? Why might that be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "2.1",
        "tags": [
          "Answer Expected"
        ],
        "id": "--D8z3-924De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465f0938-527d-4fb1-869c-a68b58569355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.6721943405275284, 0.7837150127226463)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 2.1\n",
        "print((test_macro_f1, test_micro_f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3fz09Js24De"
      },
      "source": [
        "# Inspecting Classification Results\n",
        "\n",
        "We've trained our classifier, and your final F1 should be pretty close to 1.0. Great job! But what does that mean for the languages you're actually classifying? Let's rewrite our evaluation code to allow us to look at our results instance-by-instance. For this, we're going to examine the results of a re-trained trigram classifier, trained for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zifwY1G24De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49dce6cc-6983-4888-da6d-1ab90ff5c652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1120 training observations.\n",
            "393 test observations.\n",
            "Epoch 1/1, Average Loss: 20.32535102263164\n",
            "macro-averaged F1:\t\t0.135\n",
            "micro-averaged F1:\t\t0.186\n"
          ]
        }
      ],
      "source": [
        "# Re-training a trigram classifier. Do not change this.\n",
        "\n",
        "INSPECTION_NGRAMS = 3\n",
        "\n",
        "train_set, lang_map, feature_map = preprocess_training_observations(\n",
        "    train_observations, n=INSPECTION_NGRAMS\n",
        ")\n",
        "test_set = preprocess_test_observations(\n",
        "    test_observations, feature_map, lang_map, n=INSPECTION_NGRAMS\n",
        ")\n",
        "\n",
        "random.seed(27)\n",
        "W = train(train_set, eta, epochs=1)\n",
        "\n",
        "## evaluate it as before. Check that this looks the same!\n",
        "tp, fp, fn = Counter(), Counter(), Counter()\n",
        "for ref_lang_vec, x in test_set:\n",
        "    ref_lang = np.argmax(ref_lang_vec)\n",
        "\n",
        "    hyp_lang = classify(W, x)\n",
        "    if hyp_lang == ref_lang:\n",
        "        tp[ref_lang] += 1\n",
        "    else:\n",
        "        fp[hyp_lang] += 1\n",
        "        fn[ref_lang] += 1\n",
        "# Print metrics\n",
        "\n",
        "print(f\"macro-averaged F1:\\t\\t{macro_f1(tp, fp, fn):.3f}\")\n",
        "print(f\"micro-averaged F1:\\t\\t{micro_f1(tp, fp, fn):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR1bfqAm24De"
      },
      "source": [
        "## Writing a prediction function\n",
        "\n",
        "Write a function that takes a list of observations, and produces a list of either class indices, or class names based on a parameter. This will allow you both to look at individual results from evaluating on an existing set of data (like the test set), but also for you to evaluate your classifier on new data (i.e. any string). This will involve vectorizing the list of documents, and classifying those vectors. Once that's complete, construct an inverse language mapping, from predicted indices to the language they represent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "Answer Expected"
        ],
        "id": "KcwMbq4p24De"
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    documents: list[str],\n",
        "    W: np.ndarray,\n",
        "    feature_map: dict[str, int],\n",
        "    lang_map: dict[str, np.ndarray],\n",
        "    ngram_length: int,\n",
        "    return_class_names=False,\n",
        "):\n",
        "    # Create a list (or array) of language names sorted by their order in lang_map\n",
        "    # This relies on consistent ordering, so Python 3.7+ is assumed for dict insertion order preservation\n",
        "    languages = list(lang_map.keys())\n",
        "\n",
        "    predictions = []\n",
        "    for doc in documents:\n",
        "        vectorized_doc = vectorize_document(doc, feature_map, ngram_length)\n",
        "        predicted_index = classify(W, vectorized_doc)\n",
        "\n",
        "        if return_class_names:\n",
        "            # Map the predicted index to a language name using the languages list\n",
        "            predicted_lang = languages[predicted_index] if 0 <= predicted_index < len(languages) else \"Unknown\"\n",
        "            predictions.append(predicted_lang)\n",
        "        else:\n",
        "            predictions.append(predicted_index)\n",
        "\n",
        "    return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ17un9t24De"
      },
      "source": [
        "Now, for each instance in the test set, print a tuple of the actual label, then the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "3.1",
        "tags": [
          "Answer Expected"
        ],
        "id": "88uOIVxk24De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f8d07e-a188-459f-d55e-0c53671b395a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('swahili', 'manobo'), ('indonesian', 'manobo'), ('tagalog', 'swahili'), ('indonesian', 'tagalog'), ('hausa', 'tagalog'), ('nahuatl', 'indonesian'), ('hausa', 'tagalog'), ('manobo', 'swahili'), ('swahili', 'nahuatl'), ('swahili', 'nahuatl'), ('tagalog', 'manobo'), ('indonesian', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'indonesian'), ('swahili', 'indonesian'), ('swahili', 'swahili'), ('hausa', 'swahili'), ('indonesian', 'manobo'), ('manobo', 'manobo'), ('tagalog', 'indonesian'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('manobo', 'swahili'), ('swahili', 'nahuatl'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'nahuatl'), ('nahuatl', 'hausa'), ('nahuatl', 'hausa'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('indonesian', 'tagalog'), ('nahuatl', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('tagalog', 'hausa'), ('indonesian', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'manobo'), ('swahili', 'indonesian'), ('nahuatl', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'tagalog'), ('swahili', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'manobo'), ('manobo', 'indonesian'), ('nahuatl', 'tagalog'), ('swahili', 'swahili'), ('indonesian', 'manobo'), ('nahuatl', 'nahuatl'), ('nahuatl', 'hausa'), ('tagalog', 'indonesian'), ('hausa', 'manobo'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('indonesian', 'tagalog'), ('swahili', 'nahuatl'), ('tagalog', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'hausa'), ('manobo', 'swahili'), ('tagalog', 'indonesian'), ('tagalog', 'hausa'), ('manobo', 'indonesian'), ('manobo', 'manobo'), ('manobo', 'hausa'), ('swahili', 'nahuatl'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('swahili', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'indonesian'), ('manobo', 'swahili'), ('swahili', 'manobo'), ('tagalog', 'swahili'), ('indonesian', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'manobo'), ('manobo', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'manobo'), ('indonesian', 'nahuatl'), ('indonesian', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'nahuatl'), ('tagalog', 'swahili'), ('tagalog', 'manobo'), ('hausa', 'swahili'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('nahuatl', 'tagalog'), ('tagalog', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'indonesian'), ('swahili', 'indonesian'), ('tagalog', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('tagalog', 'hausa'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('manobo', 'indonesian'), ('tagalog', 'swahili'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('manobo', 'manobo'), ('manobo', 'hausa'), ('nahuatl', 'tagalog'), ('indonesian', 'tagalog'), ('swahili', 'manobo'), ('tagalog', 'indonesian'), ('tagalog', 'manobo'), ('indonesian', 'swahili'), ('swahili', 'manobo'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('nahuatl', 'tagalog'), ('tagalog', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'nahuatl'), ('swahili', 'tagalog'), ('swahili', 'nahuatl'), ('swahili', 'nahuatl'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'indonesian'), ('tagalog', 'hausa'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('hausa', 'hausa'), ('hausa', 'nahuatl'), ('hausa', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'manobo'), ('tagalog', 'swahili'), ('manobo', 'swahili'), ('indonesian', 'swahili'), ('hausa', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'manobo'), ('manobo', 'tagalog'), ('nahuatl', 'tagalog'), ('swahili', 'indonesian'), ('tagalog', 'hausa'), ('manobo', 'indonesian'), ('tagalog', 'hausa'), ('indonesian', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'swahili'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('hausa', 'tagalog'), ('tagalog', 'indonesian'), ('manobo', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'indonesian'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('nahuatl', 'indonesian'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'indonesian'), ('tagalog', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('hausa', 'tagalog'), ('tagalog', 'manobo'), ('swahili', 'nahuatl'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'tagalog'), ('nahuatl', 'tagalog'), ('manobo', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('manobo', 'swahili'), ('tagalog', 'manobo'), ('swahili', 'manobo'), ('swahili', 'tagalog'), ('nahuatl', 'swahili'), ('nahuatl', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'hausa'), ('tagalog', 'hausa'), ('manobo', 'swahili'), ('tagalog', 'swahili'), ('hausa', 'tagalog'), ('swahili', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('hausa', 'nahuatl'), ('swahili', 'indonesian'), ('nahuatl', 'hausa'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'nahuatl'), ('hausa', 'swahili'), ('nahuatl', 'hausa'), ('swahili', 'nahuatl'), ('hausa', 'tagalog'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'nahuatl'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('nahuatl', 'swahili'), ('tagalog', 'indonesian'), ('manobo', 'nahuatl'), ('manobo', 'manobo'), ('swahili', 'indonesian'), ('nahuatl', 'indonesian'), ('nahuatl', 'swahili'), ('swahili', 'swahili'), ('hausa', 'tagalog'), ('swahili', 'swahili'), ('hausa', 'manobo'), ('tagalog', 'swahili'), ('swahili', 'indonesian'), ('swahili', 'tagalog'), ('indonesian', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'indonesian'), ('tagalog', 'swahili'), ('nahuatl', 'tagalog'), ('tagalog', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'indonesian'), ('swahili', 'indonesian'), ('swahili', 'tagalog'), ('manobo', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'tagalog'), ('indonesian', 'tagalog'), ('tagalog', 'swahili'), ('hausa', 'nahuatl'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'indonesian'), ('tagalog', 'swahili'), ('tagalog', 'indonesian'), ('indonesian', 'tagalog'), ('tagalog', 'manobo'), ('nahuatl', 'indonesian'), ('indonesian', 'indonesian'), ('manobo', 'swahili'), ('manobo', 'swahili'), ('indonesian', 'tagalog'), ('indonesian', 'tagalog'), ('tagalog', 'swahili'), ('hausa', 'hausa'), ('manobo', 'indonesian'), ('nahuatl', 'hausa'), ('swahili', 'tagalog'), ('swahili', 'manobo'), ('swahili', 'tagalog'), ('hausa', 'tagalog'), ('swahili', 'manobo'), ('nahuatl', 'hausa'), ('swahili', 'swahili'), ('nahuatl', 'hausa'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('manobo', 'nahuatl'), ('swahili', 'swahili'), ('tagalog', 'indonesian'), ('indonesian', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('nahuatl', 'hausa'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('hausa', 'manobo'), ('tagalog', 'manobo'), ('indonesian', 'manobo'), ('hausa', 'nahuatl'), ('nahuatl', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'hausa'), ('nahuatl', 'hausa'), ('swahili', 'swahili'), ('manobo', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'indonesian'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'nahuatl'), ('tagalog', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'tagalog'), ('tagalog', 'swahili'), ('indonesian', 'manobo'), ('tagalog', 'tagalog'), ('indonesian', 'swahili'), ('indonesian', 'manobo'), ('hausa', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'swahili'), ('indonesian', 'indonesian'), ('tagalog', 'indonesian'), ('tagalog', 'indonesian'), ('nahuatl', 'tagalog'), ('tagalog', 'swahili'), ('hausa', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'nahuatl'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'tagalog'), ('nahuatl', 'tagalog'), ('swahili', 'nahuatl'), ('tagalog', 'manobo'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('hausa', 'nahuatl'), ('hausa', 'manobo'), ('tagalog', 'indonesian'), ('indonesian', 'nahuatl'), ('nahuatl', 'tagalog'), ('swahili', 'tagalog'), ('manobo', 'swahili'), ('hausa', 'swahili'), ('hausa', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('nahuatl', 'hausa'), ('indonesian', 'tagalog'), ('manobo', 'swahili'), ('nahuatl', 'tagalog'), ('swahili', 'swahili'), ('indonesian', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('tagalog', 'tagalog'), ('manobo', 'swahili'), ('swahili', 'swahili'), ('manobo', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('swahili', 'nahuatl'), ('tagalog', 'swahili'), ('indonesian', 'nahuatl'), ('tagalog', 'indonesian'), ('tagalog', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'tagalog')]\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.1\n",
        "test_labels, test_documents = zip(*test_observations)\n",
        "test_predictions = predict(\n",
        "    test_documents,\n",
        "    W,\n",
        "    feature_map,\n",
        "    lang_map,\n",
        "    ngram_length=INSPECTION_NGRAMS,\n",
        "    return_class_names=True,\n",
        ")\n",
        "\n",
        "print(list(zip(test_labels, test_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvDNfv0I24De"
      },
      "source": [
        "## Plotting a Confusion Matrix\n",
        "\n",
        " A _confusion matrix_ is a $k \\times k$ matrix, where k is your number of classes, where the cell in position $(i,j)$ counts the number of instances that belong to class $i$ that were predicted to be in class $j$. The diagonal entries represent correct classifications; anything off of the diagonal represents an incorrect classification. The example below, from the [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html), shows a confusion matrix for a binary classification problem.\n",
        "\n",
        "![A confusion matrix for a binary classification problem](confusion_matrix.png)\n",
        "\n",
        "\n",
        "Confusion matrices can be useful to see what types of errors your classifier is making. If errors are concentrated into particular cells, it could indicate the kind of data that your classifier struggles with. Write a function to take a list of test observations, and output a numpy array that represents your classifier's confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKK8Yu5s24De"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix_with_labels(labels: list[str], predictions: list[str], lang_map: dict[str, np.ndarray]) -> (np.ndarray, list[str]):\n",
        "    \"\"\"\n",
        "    Generates a confusion matrix for the given labels and predictions, along with the labels for the matrix axes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : list[str]\n",
        "        The actual labels of the test observations.\n",
        "    predictions : list[str]\n",
        "        The predicted labels by the classifier.\n",
        "    lang_map : dict[str, np.ndarray]\n",
        "        A dictionary mapping language names to one-hot encoded vectors.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing the confusion matrix as a numpy array and a list of language names corresponding to the labels.\n",
        "    \"\"\"\n",
        "    # Generate a list of unique languages from lang_map keys\n",
        "    languages = list(lang_map.keys())\n",
        "    # Initialize a square matrix of zeros\n",
        "    k = len(languages)\n",
        "    confusion_matrix = np.zeros((k, k), dtype=int)\n",
        "\n",
        "    # Create a mapping from language names to indices\n",
        "    lang_to_idx = {lang: idx for idx, lang in enumerate(languages)}\n",
        "\n",
        "    # Fill the confusion matrix\n",
        "    for actual, predicted in zip(labels, predictions):\n",
        "        actual_idx = lang_to_idx[actual]\n",
        "        predicted_idx = lang_to_idx[predicted]\n",
        "        confusion_matrix[actual_idx, predicted_idx] += 1\n",
        "\n",
        "    return confusion_matrix, languages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHiwVdYp24Df"
      },
      "source": [
        "Now, print your confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "3.2",
        "tags": [
          "Answer Expected"
        ],
        "id": "2ZlRsoBh24Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975e562f-9477-4684-eab6-06b65cff6308"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 3,  0,  4,  6,  5, 10],\n",
              "        [ 1,  2,  6,  5,  3, 15],\n",
              "        [ 2,  6,  5,  2, 17,  2],\n",
              "        [13,  5,  0,  1,  3, 14],\n",
              "        [ 8, 17, 12, 18, 39, 48],\n",
              "        [ 9, 22, 15,  2, 50, 23]]),\n",
              " ['hausa', 'indonesian', 'manobo', 'nahuatl', 'swahili', 'tagalog'])"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "# the output of this cell will be used for test 3.2\n",
        "get_confusion_matrix_with_labels(test_labels, test_predictions, lang_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WK2GObn24Df"
      },
      "source": [
        "From your confusion matrix, how many Hausa examples are misclassified as Swahili? Print each of the misclassified documents on a new line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "3.3",
        "tags": [
          "Answer Expected"
        ],
        "id": "XaxC6YgG24Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3494948-6617-48d4-9d35-3c9677638ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dalilin haka kuwa sun nuna cewa ayukan da shari'a take bukata na nan a rubuce a zuciyarsu. lamirinsu kuma na yi masu shaida, tunanainsu kuma, ko dai yana kashe su, ko kuma yana karesu.\n",
            "Sai aka kawo kansa bisa tire, aka mika wa yarinyar, ta kuwa kai wa mahaifiyarta.\n",
            "An gicciye shi da 'yanfashi guda biyu, daya ta damansa dayan kuma ta hagunsa.\n",
            "Amma da hankalinsa ya dawo, ya ce, 'Barorin mahaifina su nawa ne da suke da abinci isasshe, amma ina nan a nan, ina mutuwa sabili da yunwa!\n",
            "Wannan dabban da na gani yana kama da damisa, kafafunsa kuma kamar na beyar, bakinsa kuma kamar na zaki. Wannan diragon ya ba shi karfinsa, da kursiyinsa da ikonsa mai girma na sarauta.\n"
          ]
        }
      ],
      "source": [
        "def print_misclassified_as_swahili(test_documents: list[str], test_labels: list[str], test_predictions: list[str]):\n",
        "    \"\"\"\n",
        "    Prints documents that were actually 'Hausa' but misclassified as 'Swahili'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    test_documents : list[str]\n",
        "        The list of document texts used for testing.\n",
        "    test_labels : list[str]\n",
        "        The actual labels of the test documents.\n",
        "    test_predictions : list[str]\n",
        "        The predicted labels for the test documents.\n",
        "    \"\"\"\n",
        "    for doc, actual, predicted in zip(test_documents, test_labels, test_predictions):\n",
        "        if actual == \"hausa\" and predicted == \"swahili\":\n",
        "            print(doc)\n",
        "\n",
        "# Example usage\n",
        "print_misclassified_as_swahili(test_documents, test_labels, test_predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqb2DHbe24Df"
      },
      "source": [
        "Can you formulate a hypothesis for why these Hausa examples are being misclassified as Swahili? Peruse the Wikipedia pages of the two languages, and inspect the data and features of the two languages. Consider reasons based in what you know about the languages, and about machine learning. Try to come up with 2-3 experiments you might run to validate your hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqvtMZrd24Df"
      },
      "source": [
        "## Examining Feature Weights\n",
        "\n",
        "The classifier that we've built for softmax classification behaves in many ways like using several binary softmax classifiers stacked together. The $K \\times N$ feature matrix can interpreted as a $1 \\times N$ feature vector for each class. In this section, we'll examine the weights for a few of our classified languages. To get feature names out of these vectors, we'll construct an inverted feature map, that maps indices in the feature vectors back to n-grams. Then, extract the $1 \\times N$ vector that corresponds to Hausa. Print the shape of the Hausa feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "4.1",
        "tags": [
          "Answer Expected"
        ],
        "id": "q1u6xbau24Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1b1696-c570-4337-b887-09f9e87c7e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Hausa feature vector: (7942,)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.1\n",
        "# Invert the feature map\n",
        "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
        "\n",
        "# Get the index corresponding to Hausa\n",
        "hausa_index = list(lang_map.keys()).index(\"hausa\")\n",
        "\n",
        "# Use the retrieved index to access the corresponding feature vector\n",
        "hausa_feature_vector = W[hausa_index]\n",
        "\n",
        "# Print the shape of the Hausa feature vector\n",
        "print(\"Shape of Hausa feature vector:\", hausa_feature_vector.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVe-q9gm24Df"
      },
      "source": [
        "Now, let's find the features that are most strongly predictive of an instance being Hausa. From the Hausa feature vector, find the names of the features with the top-10 positive values. Print your results with a tuple of (feature name, feature_weight) on each line:\n",
        "```\n",
        "(\"aaa\", 0.04597442104739769)\n",
        "(\"bbb\", 0.03454984682736487)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "4.2",
        "tags": [
          "Answer Expected"
        ],
        "id": "D4BHdfSL24Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c4b96e-e498-4160-de1d-44068a9e0ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ì h', 4.1119423270042645)\n",
            "('r M', 3.6340927749879994)\n",
            "('oal', 3.6260693556138626)\n",
            "('loi', 3.492575370992717)\n",
            "('kuv', 3.4843143410567725)\n",
            "('Mam', 3.314741520114097)\n",
            "('\\tKi', 3.28583461340747)\n",
            "('ini', 3.2558061675166963)\n",
            "('ika', 3.24587450133588)\n",
            "(' Di', 3.2451698945005925)\n"
          ]
        }
      ],
      "source": [
        "# the output of this cell will be used for test 4.2\n",
        "# Sort the Hausa feature vector indices based on their values\n",
        "top_indices = np.argsort(hausa_feature_vector)[::-1][:10]\n",
        "\n",
        "# Print the top 10 features along with their weights\n",
        "\n",
        "for idx in top_indices:\n",
        "    feature_name = inverted_feature_map[idx]\n",
        "    feature_weight = hausa_feature_vector[idx]\n",
        "    print((feature_name, feature_weight))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3S2YiCk24Df"
      },
      "source": [
        "Now, for each language, print the top-10 features. Print the name of each language, and then a list of it's top-10 features on the following line, e.g.\n",
        "\n",
        "```\n",
        "hausa\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "indonesian\n",
        "[(\"aaa\", 0.04597442104739769)...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output_for": "4.3",
        "tags": [
          "Answer Expected"
        ],
        "id": "KBBTNihf24Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32b0186-c3c0-43bc-866e-affa9cca9c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hausa\n",
            "[('ì h', 4.1119423270042645), ('r M', 3.6340927749879994), ('oal', 3.6260693556138626), ('loi', 3.492575370992717), ('kuv', 3.4843143410567725), ('Mam', 3.314741520114097), ('\\tKi', 3.28583461340747), ('ini', 3.2558061675166963), ('ika', 3.24587450133588), (' Di', 3.2451698945005925)]\n",
            "indonesian\n",
            "[('ixt', 3.5548251508993807), ('ko?', 3.3913554320011468), ('lom', 3.3327203235724046), ('Sob', 3.208426872896643), ('k! ', 3.1842977160757657), ('ekw', 3.135631842020595), ('otl', 3.1160906223054137), ('Nig', 3.060784719629891), (\" ''\", 3.049404139197689), ('um ', 3.04494610483891)]\n",
            "manobo\n",
            "[('ria', 3.5493881873015827), ('e-b', 3.308161448631091), (' so', 3.2859171788999726), ('Ko ', 3.187335912843954), ('om;', 3.1758739166423857), (' \"u', 3.170828804162903), ('t.”', 3.145984573427403), ('rk.', 3.115069358377994), (' dà', 3.103180373177725), ('te,', 3.02301659673324)]\n",
            "nahuatl\n",
            "[('at-', 3.4261048948110036), ('eac', 3.2705982348172613), (' Li', 3.253871248494516), ('i b', 3.2306635159016), ('“No', 3.0724937920804294), ('-u.', 3.038183066549105), ('iky', 3.0341885677949842), ('zi,', 3.0232713940351346), ('‘Ap', 2.9669298558492727), ('m’ ', 2.946347350408527)]\n",
            "swahili\n",
            "[('eem', 3.5813602630393597), ('ie;', 3.4002376790622426), ('a m', 3.38535848365986), ('gu!', 3.311600385886459), ('omb', 3.2536675929966803), ('buj', 3.1545333893969367), ('hyo', 3.145188803404633), ('siv', 3.0880706794868322), ('us.', 3.069223684795485), ('bac', 3.0144247339526653)]\n",
            "tagalog\n",
            "[('uc ', 4.044922525318122), ('Mij', 3.913613005699702), ('akt', 3.6623366783597207), ('ii.', 3.64383535355747), ('uty', 3.5839916712118427), ('ref', 3.358896245103075), ('-ke', 3.289137372892211), ('inu', 3.2701203452411036), ('i B', 3.1793075163879827), ('lfe', 3.1426121690083213)]\n"
          ]
        }
      ],
      "source": [
        "# Iterate over each language in lang_map\n",
        "for lang_name in lang_map.keys():\n",
        "    lang_index = list(lang_map.keys()).index(lang_name)\n",
        "\n",
        "    # Retrieve the feature vector for the current language\n",
        "    lang_vector = W[lang_index]\n",
        "\n",
        "    # Sort the feature vector indices based on their values\n",
        "    top_indices = np.argsort(lang_vector)[::-1][:10]\n",
        "\n",
        "    # Print the name of the current language\n",
        "    print(lang_name)\n",
        "\n",
        "    # Print the top 10 features along with their weights\n",
        "    top_features = []\n",
        "    for idx in top_indices:\n",
        "        feature_name = inverted_feature_map[idx]\n",
        "        feature_weight = lang_vector[idx]\n",
        "        top_features.append((feature_name, feature_weight))\n",
        "\n",
        "    # Print the list of top features\n",
        "    print(top_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVKpSOP224Df"
      },
      "source": [
        "Choose one of the languages, and peruse its Wikipedia page or other reliable resources to learn a bit more about the language. Do the top 10 features in that language make sense given what you've learned about the structure of the language? Why or why not? Again, consider reasons stemming both from what you've learned about the language, as well as machine learning and multinomial logistic regression specifically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GCBKLcD24Dg"
      },
      "source": [
        "## Extra Credit\n",
        "\n",
        "Implement at least one of the experiments you devised to test your hypothesis regarding misclassification of Hausa as Swahili. In order to get credit you must submit not just the code and results, but also clearly describe your hypothesis, the experiment, and why the experiment is suitable to test your hypothesis. Full credit will be given to hypotheses and experiments that are well thought out, explained clearly and convincingly, and backed up with suitable evidence (computed or cited)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApO7yWy24Dg"
      },
      "outputs": [],
      "source": [
        "# Implement extra credit here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nlphw03')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9b5c567e963c4c8a4f65edac5dbbc9d8f011f564e4ce8a416ba289a7b299ae77"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}