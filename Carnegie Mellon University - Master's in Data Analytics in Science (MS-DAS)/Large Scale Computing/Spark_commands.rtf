{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab560
\pard\pardeftab560\partightenfactor0

\f0\b\fs34 \cf0 Spark commands \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f1\b0\fs24 \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 interact\
\
module load spark\
pyspark\
\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 rdd1 = sc.textFile("pulsar.dat")\
\
rdd1.take(4) #works like .head for python\
\
rdd2 = rdd1.map(lambda x:x.split()) #splitting rdd1 by comma\
\
rdd2.take(4) #checking if rdd2 is now separated with comma\
\
\
rdd3 = rdd2.map(lambda x: [float(x[0]), float(x[1]), float(x[3])]) # taking out x,y,frequency so I can treat them as tuple, had to be float or if i use int it gives me an error\
\
rdd3.take(4) #checking that only timestamp was dropped\
\
rdd4 = rdd3.map(
\f0\b lambda
\f1\b0  x: tuple(round(val, 1) 
\f0\b for
\f1\b0  val 
\f0\b in
\f1\b0  x)) #rounding only up to 0.01, not ineger\
\
rdd4.take(4) #check if rounding to 0.1 worked\
\
rdd5 = rdd4.map(lambda x: (x,1))\
\
rdd5.take(4) #check if 1 has been added to later\
\
rdd6= rdd5.reduceByKey(lambda x,y: x+y) \
\
rdd6.take(10) #see if 1 at the end changed\
\
rdd7 = rdd6.map(lambda x: (x[1], x[0]))\
\
rdd7.take(4) #check if the count is in the front now\
\
rdd8 = rdd7.sortByKey(False)\
rdd8.take(4) #check the top order\
\
# we now know that max signal comes from 104.5, 82.1, 2231.5, we need to go back to original rdd_2 and include all \
\
rdd_x= rdd2.filter(lambda x: float(x[0]) >104.3 and float(x[0]) < 104.7)\
rdd_x.count()\
rdd_y = rdd_x.filter(lambda x: float(x[1]) >81.9 and float(x[1]) < 82.3)\
rdd_y.count()\
rdd_f = rdd_y.filter(lambda x: float(x[3]) >2231.3 and float(x[3]) < 2231.7)\
rdd_f.count()\
\
rdd_f.take(10)\
\
rdd_t = rdd_f.map(lambda x: (x[2], x[0], x[1], x[3])) #pulling the timestamp to the front\
rdd_t.take(4)\
\
rdd_t2 = rdd_t.sortByKey(True)\
\
rdd_time = rdd_t2.map(lambda x: (round(float(x[0]), 2), x[1], x[2], x[3])) #rounding the time stamp up to 0.01\
rdd_time.count() #checking how many i need to output, should be same number from rdd_f since we didn\'92t drop any \
rdd_time.take(26)\
\
\
\
\
\
\
\
signals_from_most_common_location = (locations\
.filter(lambda location: all(abs(location[i] - most_common_location[0][i]) <= tolerance for i in range(len(location)))\
)\
\
\
\
\
rdd3.take(4) #checking that only timestamp was dropped\
\
#rdd3 = rdd2.map(lambda x: [ (round(int(x[0]),1), int(x[1]), int(x[3])]) \
\
#rdd4= rdd3.map(lambda x: round(x),1 )\
#rdd4 = rdd3.map(
\f0\b lambda
\f1\b0  location: tuple(round(val, 1) 
\f0\b for
\f1\b0  val 
\f0\b in
\f1\b0  location))\
\
\
\
locations = rdd1.map(lambda fields: (float(fields[0]), float(fields[1]), float(fields[3]))) #pulling out locations x,y, and frequencies\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 location_count_tuples = locations.map(lambda location: (location, 1))\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 grouped_locations = location_count_tuples.reduceByKey(lambda a, b: a + b)\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 most_common_location = grouped_locations.max(key=lambda x: x[1])}